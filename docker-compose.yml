version: "3.9"

services:
  # ------------------ Data Lake ------------------
  minio:
    image: minio/minio
    container_name: minio-realestate
    hostname: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    command: ["server", "/data", "--console-address", ":9001"]
    volumes:
      - ./docker-volumes/minio:/data
    env_file:
      - .env
    networks:
      - data_network

  # ------------------ Hive Metastore ------------------
  hive-metastore:
    image: bitsondatadev/hive-metastore
    container_name: hive-metastore-realestate
    hostname: hive-metastore
    ports:
      - "9083:9083"
    volumes:
      - ./hive-metastore/metastore-site.xml:/opt/apache-hive-metastore-3.0.0-bin/conf/metastore-site.xml:ro
    environment:
      METASTORE_DB_HOSTNAME: mysql-airflow
    depends_on:
      - mysql-airflow
      - minio
    networks:
      - data_network

  # ------------------ Spark ------------------
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master-realestate
    ports:
      - "7077:7077"
      - "8080:8080"
    environment:
      - SPARK_MODE=master
      - SPARK_USER=spark
    volumes:
      - ./spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    networks:
      - data_network

  spark-worker-1:
    image: bitnami/spark:3.3
    container_name: spark-worker1-realestate
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-worker-2:
    image: bitnami/spark:3.3
    container_name: spark-worker2-realestate
    depends_on:
      - spark-master
    networks:
      - data_network

  spark-thrift-server:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-thrift-realestate
    restart: always
    depends_on:
      - spark-master
      - hive-metastore
    ports:
      - "10000:10000"
    command: sh -c "
      sleep 10 && ./sbin/start-thriftserver.sh \
      --hiveconf hive.metastore.uris=thrift://hive-metastore:9083 \
      --master spark://spark-master:7077"
    volumes:
      - ./spark/hive-site.xml:/opt/bitnami/spark/conf/hive-site.xml
    networks:
      - data_network

  # ------------------ Airflow ------------------
  airflow-init:
    build: ./airflow
    container_name: airflow-init-realestate
    depends_on:
      - mysql-airflow
    environment:
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql://airflow:airflow@mysql-airflow:3306/airflow
    command: bash -c "sleep 10 && airflow db init"
    networks:
      - data_network

  airflow-webserver:
    build: ./airflow
    container_name: airflow-web-realestate
    restart: always
    depends_on:
      - mysql-airflow
      - airflow-init
    environment:
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql://airflow:airflow@mysql-airflow:3306/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./datasets:/opt/airflow/datasets
    ports:
      - "8088:8080"
    command: webserver
    networks:
      - data_network

  airflow-scheduler:
    build: ./airflow
    container_name: airflow-scheduler-realestate
    restart: always
    depends_on:
      - mysql-airflow
      - airflow-init
    environment:
      - EXECUTOR=Local
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=mysql://airflow:airflow@mysql-airflow:3306/airflow
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./datasets:/opt/airflow/datasets
    command: scheduler
    networks:
      - data_network

  mysql-airflow:
    image: mysql:8.0
    container_name: mysql-airflow-realestate
    environment:
      - MYSQL_ROOT_PASSWORD=airflow
      - MYSQL_DATABASE=airflow
      - MYSQL_USER=airflow
      - MYSQL_PASSWORD=airflow
    networks:
      - data_network

  # ------------------ MLflow ------------------
  mlflow:
    build: ./mlflow
    container_name: mlflow-realestate
    ports:
      - "5000:5000"
    environment:
      - FILE_DIR=/mlflow
      - AWS_BUCKET=mlflow
      - PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python
    volumes:
      - ./mlflow:/mlflow
    networks:
      - data_network

  # ------------------ Model Serving ------------------
  model_serving:
    build: ./model_server
    container_name: model-serving-realestate
    ports:
      - "5001:5001"
    env_file:
      - .env
    networks:
      - data_network

  # ------------------ Metabase ------------------
  metabase:
    image: metabase/metabase:latest
    container_name: metabase-realestate
    ports:
      - "3000:3000"
    env_file:
      - .env
    networks:
      - data_network

  # ------------------ App Frontend/Backend ------------------
  app:
    build: ./app
    container_name: realestate-app
    ports:
      - "8051:8051"
    volumes:
      - ./app:/home/app
    networks:
      - data_network

networks:
  data_network:
    driver: bridge
